# ğŸ“˜ RAG App â€“ FastAPI + LangChain + LangGraph

A **2â€“3 day beginner-friendly RAG project** using **FastAPI**, **LangChain**, **LangGraph**, and managed with **uv**.

---

## ğŸ§± Tech Stack

* **Backend**: FastAPI
* **RAG Framework**: LangChain
* **Workflow Orchestration**: LangGraph
* **Vector Database**: Chroma
* **LLM**: OpenAI / Ollama / Groq (any one)
* **Package Manager**: uv
* **Python**: 3.10+

---

## ğŸ“ Folder Structure

```text
backend/
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ upload.py
â”‚   â”‚   â””â”€â”€ chat.py
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â””â”€â”€ config.py
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ chat.py
â”‚   â”œâ”€â”€ rag/
â”‚   â”‚   â”œâ”€â”€ loaders.py
â”‚   â”‚   â”œâ”€â”€ splitter.py
â”‚   â”‚   â”œâ”€â”€ embeddings.py
â”‚   â”‚   â”œâ”€â”€ vectorstore.py
â”‚   â”‚   â”œâ”€â”€ prompts.py
â”‚   â”‚   â””â”€â”€ graph.py
â”‚   â””â”€â”€ services/
â”‚       â””â”€â”€ ingestion.py
â”‚
â”œâ”€â”€ uploads/
â”‚   â””â”€â”€ .gitkeep
â”œâ”€â”€ .env.example
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ .python-version
â””â”€â”€ README.md
```

---

## ğŸ“¦ Dependency Setup (uv)

### Initialize project

```bash
uv init backend
cd backend
```

### Install dependencies

```bash
uv add fastapi uvicorn python-dotenv pydantic
uv add langchain langgraph
uv add langchain-openai
uv add chromadb
uv add pypdf
```

> If using Ollama instead of OpenAI:

```bash
uv add langchain-community
```

### Run the server

```bash
uv run uvicorn app.main:app --reload
```

---

## ğŸ” Environment Variables (`.env`)

```env
OPENAI_API_KEY=sk-xxxx
```

---

## ğŸ—“ï¸ Day-wise Implementation Plan

---

## âœ… Day 1 â€“ Document Ingestion (Core RAG)

**Goal**: Upload documents and store embeddings in a vector database.

### Tasks

* Setup FastAPI app and routing
* File upload endpoint
* Load documents (PDF / TXT)
* Chunk text
* Generate embeddings
* Store embeddings in Chroma

### Files to implement

* `app/main.py`
* `api/upload.py`
* `services/ingestion.py`
* `rag/loaders.py`
* `rag/splitter.py`
* `rag/embeddings.py`
* `rag/vectorstore.py`

ğŸ¯ **Outcome**: Documents are ingested and searchable

---

## âœ… Day 2 â€“ Querying + LangGraph

**Goal**: Ask questions and get answers grounded in documents.

### Tasks

* Retrieve relevant chunks from vector DB
* Create prompt templates
* Build LangGraph workflow
* Generate answers using LLM
* Return source documents

### Files to implement

* `rag/prompts.py`
* `rag/graph.py`
* `api/chat.py`
* `models/chat.py`

ğŸ¯ **Outcome**: RAG pipeline works end-to-end

---

## âœ… Day 3 â€“ Polish & Demo (Optional)

**Goal**: Make the project demo-ready.

### Tasks

* Simple frontend (HTML/JS or React)
* Show source chunks
* Handle no-context answers
* Improve prompts
* Add logging / error handling

ğŸ¯ **Outcome**: Clean demo suitable for GitHub & portfolio

---

## ğŸ” LangGraph Workflow

```text
User Query
   â†“
Retrieve Documents
   â†“
(Optional) Relevance Filtering
   â†“
Generate Answer
   â†“
Return Answer + Sources
```

LangGraph Nodes:

* `retrieve_docs`
* `filter_docs` (optional)
* `generate_answer`

---

## ğŸš€ Stretch Features (Optional)

* Multi-user document namespaces
* Conversation memory
* Streaming responses
* Feedback loop
* Caching

---

## ğŸ“Œ Best Practices

* Keep API routes thin
* Isolate all RAG logic inside `rag/`
* Use `services/` for workflows
* Never hardcode secrets

---

## ğŸ Final Outcome

A clean, production-style **RAG backend** that:

* Uses LangGraph correctly
* Is realistic but beginner-friendly
* Can be completed in **2â€“3 days**
* Looks strong on a resume and GitHub
